<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta property="og:site_name" content="Lettier">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Linear Regression and the Amazing Beard by David Lettier">
    <meta property="og:image" content="https://lettier.github.io/images/2017-01-15-linear-regression-and-the-amazing-beard/jumbotron_image.jpg">
    <meta property="og:url" content="https://lettier.github.io/posts/2017-01-15-linear-regression-and-the-amazing-beard.html">
    <meta property="og:description" content="Using PureScript, Halogen, and Chart.js, we implement simple linear regression, using the gradient descent algorithm, from scratch.">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="David Lettier">
    <meta name="description" content="Using PureScript, Halogen, and Chart.js, we implement simple linear regression, using the gradient descent algorithm, from scratch.">
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css">
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
    <link rel="stylesheet" type="text/css" href="../css/pandoc.css">
    <link rel="stylesheet" type="text/css" href="../css/default.css">
    <link rel="alternate" type="application/rss+xml" href="../rss.xml" title="RSS">
    
      <title>Linear Regression and the Amazing Beard by David Lettier</title>
    
  </head>
  <body>
    <div id="top"></div>
    <nav class="navbar navbar-inverse navbar-transparent navbar-fixed-top">
      <div class="container-fluid navbar-container">
        <ul class="nav navbar-nav nav-left">
          <!--
          <li class="nav-link"><a href="/">Home</a></li>
          <li class="nav-link"><a href="/posts.html">Posts</a></li>
          -->
          <li>
            <div class="nav-icon-container">
              <a href="../" title="Home">
                <div>
                  <i class="fa fa-home nav-icon"></i>
                </div>
                <div class="nav-icon-dot">
                </div>
              </a>
            </div>
          </li>
          <li>
            <div class="nav-icon-container">
              <a href="../rss.xml" title="RSS" type="application/rss+xml" target="_blank">
                <div>
                  <i class="fa fa-rss nav-icon"></i>
                </div>
                <div class="nav-icon-dot">
                </div>
              </a>
            </div>
          </li>
          <li>
            <div class="nav-icon-container">
              <a href="../posts.html" title="Posts">
                <div>
                  <i class="fa fa-th-large nav-icon shaker"></i>
                </div>
                <div class="nav-icon-dot">
                </div>
              </a>
            </div>
          </li>
        </ul>
        <div class="nav-right">
          <!--
          <a href="http://www.lettier.com/" title="Lettier.com">
            <div class="lettier-icon">
              <img src="/images/logo.svg" width="30" height="30" alt="Lettier.com" class="lettier-icon-img">
            </div>
          </a>
          -->
          <div class="nav-icon-container">
            <a href="http://www.lettier.com/" title="Lettier.com">
              <div class="lettier-icon">
                <img src="../images/logo.svg" width="30" height="30" alt="Lettier.com" class="lettier-icon-img">
              </div>
              <div class="nav-icon-dot">
              </div>
            </a>
          </div>
          <!--
          <a href="https://www.github.com/lettier" title="Github">
            <i class="fa fa-github nav-icon"></i>
          </a>
          -->
          <div class="nav-icon-container">
            <a href="https://www.github.com/lettier" title="Github">
              <div>
                <i class="fa fa-github nav-icon"></i>
              </div>
              <div class="nav-icon-dot">
              </div>
            </a>
          </div>
          <!--
          <a href="https://www.linkedin.com/in/lettier" title="LinkedIn">
            <i class="fa fa-linkedin nav-icon"></i>
          </a>
          -->
          <div class="nav-icon-container">
            <a href="https://www.linkedin.com/in/lettier" title="LinkedIn">
              <div>
                <i class="fa fa-linkedin nav-icon"></i>
              </div>
              <div class="nav-icon-dot">
              </div>
            </a>
          </div>
          <!--
          <a href="https://stackoverflow.com/users/3838674/lettier" title="Stack Overflow">
            <i class="fa fa-stack-overflow nav-icon"></i>
          </a>
          <div class="nav-icon-container">
            <a href="https://stackoverflow.com/users/3838674/lettier" title="Stack Overflow">
              <div>
                <i class="fa fa-stack-overflow nav-icon"></i>
              </div>
              <div class="nav-icon-dot">
              </div>
            </a>
          </div>
          -->
          <!--
          <a href="https://www.hackerrank.com/lettier" title="HackerRank">
            <i class="fa fa-trophy nav-icon nav-icon-second-last"></i>
          </a>
          -->
          <div class="nav-icon-container nav-icon-container-second-last">
            <a href="https://www.hackerrank.com/lettier" title="HackerRank">
              <div>
                <i class="fa fa-trophy nav-icon nav-icon-second-last"></i>
              </div>
              <div class="nav-icon-dot">
              </div>
            </a>
          </div>
          <!--
          <a href="https://www.behance.net/dlettier" title="Behance">
            <i class="fa fa-behance nav-icon nav-icon-last"></i>
          </a>
          -->
          <div class="nav-icon-container nav-icon-container-last">
            <a href="https://www.behance.net/dlettier" title="Behance">
              <div>
                <i class="fa fa-behance nav-icon"></i>
              </div>
              <div class="nav-icon-dot">
              </div>
            </a>
          </div>
        </div>
      </div>
    </nav>
    <div class="jumbotron" style="background-image: url('/images/2017-01-15-linear-regression-and-the-amazing-beard/jumbotron_image.jpg');">
      <div class="container vertical-center">
        <div class="jumbotron-text">
          <h1 class="jumbotron-text-background">
            
              Linear Regression and the Amazing Beard
            
          </h1>
        </div>
      </div>
    </div>
    <div class="container page-container">
      <div class="post-header">
  <div class="display-left">
  </div>
  <div class="display-right date-author">
    <p>2017-01-15 &nbsp; <i class="fa fa-calendar"></i></p>
    
      <p>David Lettier &nbsp; <i class="fa fa-user"></i></p>
    
  </div>
</div>
<div class="post-body">
  <!--https://pixabay.com/en/ball-glass-about-reflection-625908/-->
<h2 id="demo-and-codebase">Demo and Codebase</h2>
<p>To play around with building a simple linear regression model in your browser, try out the visually interactive <a href="https://lettier.com/linear-regression/">demo</a>. All of the code for the demo is hosted on <a href="https://github.com/lettier/interactive-simple-linear-regression">GitHub</a>. Stars are always appreciated.</p>
<h2 id="the-request">The Request</h2>
<p><img src="../images/2017-01-15-linear-regression-and-the-amazing-beard/request.jpg" alt="The SideShow" class="post-img post-img-fill" /> <!--https://pixabay.com/en/circus-tarot-seer-742054/--></p>
<p>As you put away your crystal ball for the evening, the ringmaster steps in. “We need you to foretell how long Hans’ beard will be in the future.” You being the fortune teller, the ringmaster assumes this is directly in your wheelhouse. “Right now it is 17 feet long but I want marketing to start saying it is 18, 19, 20 whatever. Since you’re never wrong, no one will be any the wiser by the time we reach our next venue.”</p>
<p>Hans has an amazing beard—probably the longest in the world—and it always gathers a crowd. You see marketing is always one stop ahead. They drum up anticipation so that when the sideshow finally comes, everyone cannot wait to see all of the marvels.</p>
<p>The idea is for you to predict how long Hans’ beard will be at the moment in time when the show reaches the next stop. This way they can market the future length now, in the next town over, and not be found a fraud by the time Hans arrives. “The bigger the number, the larger the crowd,” says the ringmaster.</p>
<p>You are given some historical data. The independent variable is how many months it has been since they started recording the length. The dependent variable is how long Hans’ beard was at the time of measurement. Note that every stop usually lasts about a month long.</p>
<div class="sourceCode"><pre class="sourceCode javascript"><code class="sourceCode javascript"><span class="op">|</span> Month <span class="op">|</span> Length <span class="op">|</span>
<span class="op">|-------|--------|</span>
<span class="op">|</span> <span class="dv">0</span>     <span class="op">|</span> <span class="fl">5.3</span>    <span class="op">|</span>
<span class="op">|</span> <span class="dv">1</span>     <span class="op">|</span> <span class="fl">8.3</span>    <span class="op">|</span>
<span class="op">|</span> <span class="dv">2</span>     <span class="op">|</span> <span class="fl">11.8</span>   <span class="op">|</span>
<span class="op">|</span> <span class="dv">3</span>     <span class="op">|</span> <span class="fl">11.9</span>   <span class="op">|</span>
<span class="op">|</span> <span class="dv">4</span>     <span class="op">|</span> <span class="fl">13.2</span>   <span class="op">|</span>
<span class="op">|</span> <span class="dv">5</span>     <span class="op">|</span> <span class="fl">12.1</span>   <span class="op">|</span>
<span class="op">|</span> <span class="dv">6</span>     <span class="op">|</span> <span class="fl">15.8</span>   <span class="op">|</span>
<span class="op">|</span> <span class="dv">7</span>     <span class="op">|</span> <span class="fl">16.2</span>   <span class="op">|</span>
<span class="op">|</span> <span class="dv">8</span>     <span class="op">|</span> <span class="fl">17.01</span>  <span class="op">|</span>
<span class="op">|</span> <span class="dv">9</span>     <span class="op">|</span> <span class="dv">17</span>     <span class="op">|</span></code></pre></div>
<h2 id="plotting-the-data">Plotting the Data</h2>
<p>Recently you’ve been taking a few MOOCs with the hope of becoming a data scientist. Fortune telling doesn’t pay what it used to and your cousin says data science is a pretty hot right now.</p>
<div class="figure">
<img src="../images/2017-01-15-linear-regression-and-the-amazing-beard/data_plot.jpg" class="post-img post-img-small post-img-limit" />

</div>
<p>Not sure where to start, you decide to plot the data to get a feel for it.</p>
<div class="figure">
<img src="../images/2017-01-15-linear-regression-and-the-amazing-beard/data_plot_string.jpg" class="post-img post-img-small post-img-limit" />

</div>
<p>As you study the dots, a string from your robe falls and lays ever so slighty across the dots.</p>
<div class="figure">
<img src="../images/2017-01-15-linear-regression-and-the-amazing-beard/data_plot_string_swivel.jpg" class="post-img post-img-small post-img-limit" />

</div>
<p>You notice the string gives the dots an up-and-to-the-right type trajectory. Pulling it taught, you swivel the string through the dots trying to cross as many as possible.</p>
<h2 id="hypothesis-function">Hypothesis Function</h2>
<p>As you play with the string, you get the idea that you could some how <em>summarize</em> the data with only the string. Why not use the equation for a straight line you think to yourself. This equation could model the data. Not only could you use it to summarize but you could also use it to predict future data points with only a single function.</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="co">-- y = mx + b</span>
<span class="ot">hypothesis ::</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Number</span>
hypothesis yIntercept slope x <span class="fu">=</span> yIntercept <span class="fu">+</span> slope <span class="fu">*</span> x</code></pre></div>
<p>We will use PureScript to program our solution into the computer. If you have ever used Haskell, the syntax is very similar. If you have not used Haskell, don’t worry as the code will be very readable.</p>
<blockquote>
PureScript is a small strongly typed programming language that compiles to JavaScript.
<footer>
<a href="http://purescript.org">PureScript.org</a>
</footer>
</blockquote>
<p>This is our hypothesis function which is the point-slope form of the equation for a straight line. Our hypothesis is that the equation of a straight line will best model for the data we were given. This line will cross the y-axis at the <code>yIntercept</code> when <code>x</code> is zero. If the <code>slope</code> is zero, the line is horizontal at the <code>yIntercept</code>.</p>
<h2 id="cost-function">Cost Function</h2>
<p>You know that a line will work well but which line? Any straight line you choose will not intersect all of the points. So every possible line will be <em>off</em> in some way.</p>
<p>What you really need to figure out is what y-intercept and slope to use. You want to find the best parameters such that the resulting line will be the least off out of all the possible straight line choices.</p>
<p>Just then the ringleader steps into your tent, “Just so you know, if I find out you’re wrong, I’ll have to dock your pay.” “Oh great,” you think to yourself, “If I don’t pick the best line, it will cost me.”</p>
<p>You propose a cost function as a way to model how far off your hypothesis function is.</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">cost ::</span> (<span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Number</span>) <span class="ot">-&gt;</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Array</span> (<span class="dt">Array</span> <span class="dt">Number</span>) <span class="ot">-&gt;</span> <span class="dt">Number</span>
cost _ _ _ [] <span class="fu">=</span> <span class="fl">0.0</span>
cost hypothesisFunc yIntercept slope values <span class="fu">=</span> (sum <span class="fu">&lt;&lt;&lt;</span> map error) values <span class="fu">/</span> size
  <span class="kw">where</span>
<span class="ot">    size ::</span> <span class="dt">Number</span>
    size <span class="fu">=</span> (toNumber <span class="fu">&lt;&lt;&lt;</span> length) values
<span class="ot">    error ::</span> <span class="dt">Array</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Number</span>
    error <span class="fu">=</span> arrayNumberHandler error'
<span class="ot">    error' ::</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Number</span>
    error' x y <span class="fu">=</span> pow (y <span class="fu">-</span> (hypothesisFunc yIntercept slope x)) <span class="fl">2.0</span></code></pre></div>
<p>Our cost function takes in four parameters:</p>
<ul>
<li>our hypothesis function</li>
<li>y-intercept choice</li>
<li>slope choice</li>
<li><code>(x,y)</code> coordinates or <code>(month, beard length)</code></li>
</ul>
<p>For any given input point, our error is:</p>
<div class="sourceCode"><pre class="sourceCode markdown"><code class="sourceCode markdown">(predicted value y' returned from our hypothesis function - actual value y)^2</code></pre></div>
<p>We do this for every input point—square the difference between the actual value and the predicted value given by our hypothesis function. Taking all of these squared prediction errors, we sum them up. This sum is then divided by the total number of points. In other words, we compute the <em>mean squared error</em> (MSE). This is our statistic for how far off (the quality of our estimator) our hypothesis function (and the choice of its parameters) are.</p>
<blockquote>
An MSE of zero, meaning that the estimator predicts observations of the parameter θ with perfect accuracy, is the ideal, but is typically not possible.
<footer>
<a href="https://en.wikipedia.org/wiki/Mean_squared_error">Mean Squared Error - Wikipedia</a>
</footer>
</blockquote>
<h2 id="gradient-descent">Gradient Descent</h2>
<p>So you have a way to make a straight line and you have a way to judge the quality of this line but the question still remains—which line? Stumped, you go to the ringleader and ask for more time.</p>
<p>As dawn arrives, the entire troupe sets out for the next town. The road is especially hilly. “The grade is too steep!” the elephant handler yells out, “We need to wind around.” When you reach the bottom of a valley—it dawns on you—you just need to pick the y-intercept and slope that minimizes your cost function the most.</p>
<p>Thinking back to your math classes, you remember that you’ve found–at the very least–a local minimum (or maximum) when the derivative of a function equals zero since it is neither decreasing or increasing. You decide to derive the partial derivatives of your cost function with respect to the y-intercept (<code>b</code>) and the slope (<code>m</code>).</p>
<div class="sourceCode"><pre class="sourceCode markdown"><code class="sourceCode markdown">Partial Derivative of the Cost Function with respect to the y-intercept (b):

d/db = (1/2N)     * sum[(    y'       - y)^2 ]
<span class="bn">     = (1/2N)     * sum[(    (mx + b) - y)^2 ] # Expand our Hypothesis Function</span>
<span class="bn">     = (1/2N)     * sum[     (mx + b  - y)^2 ]</span>
<span class="bn">     = (1/2N)     * sum[2 *  (mx + b  - y)   ] # Chain Rule -- x, m, and y are constants</span>
<span class="bn">     = (2 * 1/2N) * sum[     (mx + b  - y)   ] # Cancel Out the 2</span>
<span class="bn">     = (1/N)      * sum[     (mx + b  - y)   ]</span>

Partial Derivative of the Cost Function with respect to the slope (m):

d/dm = (1/2N)     * sum[(   y'       - y)^2  ]
<span class="bn">     = (1/2N)     * sum[(   (mx + b) - y)^2  ] # Expand our Hypothesis Function</span>
<span class="bn">     = (1/2N)     * sum[    (mx + b  - y)^2  ]</span>
<span class="bn">     = (1/2N)     * sum[2 * (mx + b  - y) * x] # Chain Rule -- x, b, and y are constants</span>
<span class="bn">     = (2 * 1/2N) * sum[    (mx + b  - y) * x] # Cancel Out the 2</span>
<span class="bn">     = (1/N)      * sum[    (mx + b  - y) * x]</span></code></pre></div>
<p>The <code>2</code> in <code>(1/2N)</code> is added in some derivations. The <code>m</code> and <code>b</code> that minimize <code>(1/N) * sum[(y' - y)^2]</code> will also minimize <code>(1/2N) * sum[(y' - y)^2]</code>. You’ll notice that it does simplify the derivatives.</p>
<p>With the two derivatives in hand, we can now look at using the gradient <code>&lt;d/db, d/dm&gt;</code> of the cost function in our attempt to find its minimum.</p>
<blockquote>
Consider a surface whose height above sea level at point (x, y) is H(x, y). The gradient of H at a point is a vector pointing in the direction of the steepest slope or grade at that point. The steepness of the slope at that point is given by the magnitude of the gradient vector.
<footer>
<a href="https://en.wikipedia.org/wiki/Mean_squared_error">Gradient - Wikipedia</a>
</footer>
</blockquote>
<p>Given a point on the cost function, the gradient points in the direction of the greatest increase. Thus we want to head in the opposite direction of the gradient. This will allow us to either reach a local minimum or the global minimum of our cost function.</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">gradientForYIntercept ::</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Array</span> (<span class="dt">Array</span> <span class="dt">Number</span>) <span class="ot">-&gt;</span> <span class="dt">Number</span>
gradientForYIntercept yIntercept slope <span class="fu">=</span> gradient innerSum
  <span class="kw">where</span>
<span class="ot">    innerSum ::</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Number</span>
    innerSum x y <span class="fu">=</span> (yIntercept <span class="fu">+</span> slope <span class="fu">*</span> x) <span class="fu">-</span> y <span class="co">-- d/db = (1/N) * sum[(mx + b - y)]</span>

<span class="ot">gradientForSlope ::</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Array</span> (<span class="dt">Array</span> <span class="dt">Number</span>) <span class="ot">-&gt;</span> <span class="dt">Number</span>
gradientForSlope yIntercept slope <span class="fu">=</span> gradient innerSum
  <span class="kw">where</span>
<span class="ot">    innerSum ::</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Number</span>
    innerSum x y <span class="fu">=</span> ((yIntercept <span class="fu">+</span> slope <span class="fu">*</span> x) <span class="fu">-</span> y) <span class="fu">*</span> x <span class="co">-- d/dm = (1/N) * sum[(mx + b - y) * x]</span>

<span class="ot">gradient ::</span> (<span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Number</span>) <span class="ot">-&gt;</span> <span class="dt">Array</span> (<span class="dt">Array</span> <span class="dt">Number</span>) <span class="ot">-&gt;</span> <span class="dt">Number</span>
gradient f values <span class="fu">=</span> (<span class="fl">1.0</span> <span class="fu">/</span> size) <span class="fu">*</span> (sum <span class="fu">&lt;&lt;&lt;</span> map innerSum) values
  <span class="kw">where</span>
<span class="ot">    size ::</span> <span class="dt">Number</span>
    size <span class="fu">=</span> (toNumber <span class="fu">&lt;&lt;&lt;</span> length) values
<span class="ot">    innerSum ::</span> <span class="dt">Array</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Number</span>
    innerSum <span class="fu">=</span> arrayNumberHandler f</code></pre></div>
<p>These functions make up the two derivatives listed above. All together, they constitute the gradient of our cost function. We can use these to find better and better (more optimal) y-intercepts and slopes.</p>
<div class="figure">
<img src="../images/2017-01-15-linear-regression-and-the-amazing-beard/gradient_descent.svg" class="post-img post-img-small post-img-limit" />

</div>
<p>As we travel along the cost function (looking for the global minimum), descending against the gradient, we must decide on how big of next step we will take. Take too big of a step and you may just overshoot the minimum. Take too small of a step and it may take forever to reach the minimum. This is known as the <em>learning rate</em>.</p>
<p>To keep it simple, you can maintain a constant learning rate throughout the duration of gradient descent. However this may cause some problems depending on your cost function landscape. In the <a href="https://github.com/lettier/interactive-simple-linear-regression/blob/master/src/Main.purs#L206">demo</a> we employ a technique known as <a href="http://www.willamette.edu/~gorr/classes/cs449/momrate.html">Bold Driver</a>. Ideally we’d like to make leaps and bounds the farther away we are from the minimum until we get very near. Once we start getting close, we want to take smaller and smaller steps until we slowly slide right into the minimum.</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">newYIntercept ::</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Array</span> (<span class="dt">Array</span> <span class="dt">Number</span>) <span class="ot">-&gt;</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Number</span>
newYIntercept learningRate values oldYIntercept slope <span class="fu">=</span> newYIntercept'
  <span class="kw">where</span>
<span class="ot">    gradientForYIntercept' ::</span> <span class="dt">Number</span>
    gradientForYIntercept' <span class="fu">=</span> gradientForYIntercept oldYIntercept slope values
<span class="ot">    newYIntercept' ::</span> <span class="dt">Number</span>
    newYIntercept' <span class="fu">=</span> oldYIntercept <span class="fu">-</span> (learningRate <span class="fu">*</span> gradientForYIntercept')

<span class="ot">newSlope ::</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Array</span> (<span class="dt">Array</span> <span class="dt">Number</span>) <span class="ot">-&gt;</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Number</span>
newSlope learningRate values yIntercept oldSlope <span class="fu">=</span> newSlope'
  <span class="kw">where</span>
<span class="ot">    gradientForSlope' ::</span> <span class="dt">Number</span>
    gradientForSlope' <span class="fu">=</span> gradientForSlope yIntercept oldSlope values
<span class="ot">    newSlope' ::</span> <span class="dt">Number</span>
    newSlope' <span class="fu">=</span> oldSlope <span class="fu">-</span> (learningRate <span class="fu">*</span> gradientForSlope')</code></pre></div>
<p>These functions round out our gradient descent algorithm. We take an initial slope and y-intercept, compute its gradient component, multiply the gradient component by the learning rate, take this product minus the initial value, and arrive at our new value.</p>
<p>To recap:</p>
<ul>
<li>Make some guess as to what the y-intercept and slope is</li>
<li>Compute y-intercept’s and slope’s respective gradient component</li>
<li>For each component, multiply it by the learning rate</li>
<li>Subtract from the old value the learning rate multiplied by the gradient component</li>
<li>Return this newly updated value</li>
<li>Repeatedly feed the cost function the updated y-intercepts and slopes (along with the data points)</li>
<li>Stop when you’ve either reached convergence (the global or a local minimum) or some threshold</li>
</ul>
<p>Once we have (hopefully) reached the (possibly global) minimum of the cost function, the most optimal line for modeling our data emerges. In other words, we minimized the cost function as best we could by repeatedly using the gradient to update our y-intercept and slope guesses. The final y-intercept and slope can be plugged into our hypothesis function giving us our complete simple linear regression model. It is here that we have <em>machine learned</em> what the y-intercept and slope needs to be.</p>
<h2 id="simple-linear-regression">Simple Linear Regression</h2>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">runLinearRegression ::</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">Array</span> (<span class="dt">Array</span> <span class="dt">Number</span>) <span class="ot">-&gt;</span> <span class="dt">Array</span> <span class="dt">Number</span>
runLinearRegression _ _ _ _ yIntercept slope [] <span class="fu">=</span> [yIntercept, slope] <span class="co">-- if the values are empty, return what they passed</span>
runLinearRegression <span class="co">-- otherwise perform Linear Regression</span>
  currentIteration
  maxIterations
  maxCost
  learningRate
  yIntercept
  slope
  values
  <span class="fu">=</span>
    <span class="kw">if</span> cost' <span class="fu">&lt;=</span> maxCost <span class="fu">||</span> currentIteration <span class="fu">&gt;=</span> maxIterations
      <span class="kw">then</span> [newYIntercept', newSlope']
      <span class="kw">else</span> runLinearRegression (currentIteration <span class="fu">+</span> <span class="dv">1</span>) maxIterations maxCost learningRate newYIntercept' newSlope' values
  <span class="kw">where</span>
    newYIntercept' <span class="fu">=</span> newYIntercept learningRate values yIntercept slope
    newSlope' <span class="fu">=</span> newSlope learningRate values yIntercept slope
    cost' <span class="fu">=</span> cost hypothesis newYIntercept' newSlope' values</code></pre></div>
<p>This function puts it all together. We start off with the current iteration, a cap on iterations, a cost threshold, the learning rate, the initial y-intercept guess, the initial slope guess, and the x y coordinates. If the cost is less than our threshold or if the current iteration is higher than the max, return the learned y-intercept and slope. These you can feed into the hypothesis function, along with any x value, to output a y prediction. If the cost is not less than our threshold and the current iteration is not higher than the max, call this function again with an increased iteration count, the <code>newYIntercept</code>, and the <code>newSlope</code>. We may never get our ideal cost (0.0), so we cap the number of iterations. If we didn’t, this function would continously call itself. Note that the recursive call is at the <em>tail</em>. This way we won’t reach the max stack depth.</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="fu">&gt;</span> <span class="kw">import </span><span class="dt">LinearRegression</span>
<span class="fu">&gt;</span> runLinearRegression <span class="dv">0</span> <span class="dv">100000</span> 1e<span class="fu">-</span><span class="dv">11</span> <span class="fl">0.0005</span> <span class="fl">0.0</span> <span class="fl">0.0</span> [[<span class="fl">0.0</span>, <span class="fl">0.0</span>], [<span class="fl">1.0</span>, <span class="fl">1.0</span>], [<span class="fl">2.0</span>, <span class="fl">2.0</span>], [<span class="fl">3.0</span>, <span class="fl">3.0</span>], [<span class="fl">4.0</span>, <span class="fl">4.0</span>], [<span class="fl">5.0</span>, <span class="fl">5.0</span>]]
[<span class="fl">0.000005599254920274064</span>,<span class="fl">0.9999984220684297</span>]</code></pre></div>
<p>This is how you would call <code>runLinearRegression</code>. As a sanity check, we can pass it some points that are on the line <code>y = 1 * x + 0</code> (the y-intercept is zero and the slope is one). As you can see, the result <code>y = 0.999998 * x + 0.000005</code> is very close to the actual line that generated the input points.</p>
<div class="figure">
<img src="../images/2017-01-15-linear-regression-and-the-amazing-beard/found_y_intercept_slope.jpg" class="post-img post-img-small post-img-limit" />

</div>
<p>As you make your way to town, you rush over to the ringmaster. “I have my prediction,” you say, “Hans’ beard will be 1.2 * 10 + 7.4 = 19.4 feet long next mont…” However, before you could get another word out, the fire breather walks by, blurting out, “There’s a closed form solution you know. You didn’t need to use gradient descent.”</p>
<h2 id="wrap-up">Wrap-up</h2>
<p>We discussed the details of building a simple linear regression model (from scratch) using a fictitious story about predicting the length of a beard. Along the way, we looked at how each step is implemented in the <a href="http://lettier.com/linear-regression/">interactive demo</a> built with PureScript, <a href="https://github.com/slamdata/purescript-halogen">PureScript-Halogen</a>, and <a href="http://www.chartjs.org/">Chart.js</a>.</p>
<p>Now that you’ve seen how simple linear regression via gradient descent works, take a look at some other machine learning algorithms such as <a href="../posts/2016-06-10-k-nearest-neighbors-from-scratch.html">k-Nearest Neighbors</a> and <a href="../posts/2016-04-24-k-means-from-scratch.html">K-Means</a>.</p>
</div>
<div class="post-footer">
  <div class="display-left">
    <h2 class="post-footer-cta">
      Don't miss out&mdash;click <a href="../posts.html" title="posts">posts</a> or
      subscribe via <a href="../rss.xml" type="application/rss+xml" target="_blank" title="RSS">RSS</a> for more content.
      <!--Return to the <a href="#top" title="top">top</a>.-->
    </h2>
  </div>
  <div class="display-right text-align-right post-footer-copyright">
    <h4>
      <i class="fa fa-copyright"></i> <span id="copyrightYear">2015</span> David Lettier.
    </h4>
  </div>
</div>
<script>
  (function () {
    document.getElementById('copyrightYear').innerHTML = new Date().getFullYear();
  })();
</script>

    </div>
    <div class="share-toolbox addthis_sharing_toolbox"></div>
    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.12.0/jquery.min.js"></script>
    <script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-4fc2bc7a00a9352b"></script>
    <script>
      (function(i,s,o,g,r,a,m){
        i.GoogleAnalyticsObject = r;
        i[r] = i[r] || function () { (i[r].q = i[r].q || []).push(arguments); };
        i[r].l=1*new Date();
        a=s.createElement(o);
        m=s.getElementsByTagName(o)[0];
        a.async=1;
        a.src=g;
        m.parentNode.insertBefore(a,m);
      })(
        window,
        document,
        'script',
        '//www.google-analytics.com/analytics.js',
        'ga'
      );
      ga('create', 'UA-34323684-2', 'auto');
      ga('send', 'pageview');
    </script>
  </body>
</html>
